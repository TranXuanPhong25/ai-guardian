#!/usr/bin/env python3
"""
Accurate PII Evaluation System - ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c d·ª±a tr√™n ground truth
"""

import csv
import json
import random
import asyncio
from typing import Dict, List, Tuple, Set
from datetime import datetime
from faker import Faker
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from app.services.masking_service import PIIMaskerService
    from app.services.unmasking_service import PIIUnmaskerService
except ImportError as e:
    print(f"Error importing services: {e}")
    sys.exit(1)


class AccuratePIIEvaluator:
    """
    ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c PII d·ª±a tr√™n ground truth c√≥ s·∫µn
    - T·∫°o dataset v·ªõi PII ƒë∆∞·ª£c ƒë√°nh d·∫•u s·∫µn
    - ƒêo l∆∞·ªùng Precision, Recall, F1-Score ch√≠nh x√°c
    - ƒê√°nh gi√° kh·∫£ nƒÉng masking/unmasking
    - Filtering false positives th√¥ng minh
    """
    
    def __init__(self):
        self.faker = Faker(['en_US', 'vi_VN'])
        self.masker = PIIMaskerService()
        self.unmasker = PIIUnmaskerService()
        
        # Entities th·∫≠t s·ª± quan tr·ªçng (filter noise)
        self.relevant_entities = [
            'PERSON', 'EMAIL_ADDRESS', 'PHONE_NUMBER', 
            'CREDIT_CARD', 'US_SSN', 'IBAN_CODE'
        ]
        
        # Minimum confidence threshold
        self.min_confidence = 0.5
    
    def filter_detections(self, analysis_results, text: str):
        """Filter out low-confidence and irrelevant detections"""
        filtered = []
        
        for result in analysis_results:
            # Filter by entity type relevance
            if result.entity_type not in self.relevant_entities:
                continue
                
            # Filter by confidence
            if result.score < self.min_confidence:
                continue
            
            # Filter specific false positive patterns
            entity_text = text[result.start:result.end]
            
            # Skip obvious invoice/stock codes
            if result.entity_type == 'US_DRIVER_LICENSE':
                if entity_text.startswith('P') or entity_text.startswith('536'):
                    continue
            
            # Skip date detection if in InvoiceDate context
            if result.entity_type == 'DATE_TIME':
                context_start = max(0, result.start - 20)
                context = text[context_start:result.start]
                if 'InvoiceDate' in context:
                    continue
            
            filtered.append(result)
        
        return filtered

    def create_labeled_dataset(self, num_records: int = 500) -> Tuple[List[Dict], Dict]:
        """
        T·∫°o dataset v·ªõi ground truth labels
        Returns:
            - List of records v·ªõi PII ƒë∆∞·ª£c ƒë√°nh d·∫•u
            - Ground truth mapping {record_id: [(entity_type, value, start, end)]}
        """
        records = []
        ground_truth = {}
        
        print(f"Creating labeled dataset with {num_records} records...")
        
        for i in range(num_records):
            record = {
                'InvoiceNo': f"{536365 + i}",
                'StockCode': f"P{random.randint(1000, 9999)}",
                'Description': random.choice(['LIKE', 'PRESENT', 'GIFT', 'ITEM']),
                'Quantity': random.randint(1, 50),
                'InvoiceDate': self.faker.date_time_between(start_date='-1y', end_date='now').strftime('%Y-%m-%d %H:%M:%S'),
                'UnitPrice': round(random.uniform(1.0, 100.0), 2),
                'CustomerID': random.randint(10000, 99999),
                'Country': self.faker.country()
            }
            
            # Danh s√°ch PII th·∫≠t s·ª± c√≥ trong record n√†y
            true_pii = []
            
            # Th√™m PII c√≥ ch·ªß √Ω v·ªõi x√°c su·∫•t cao h∆°n
            if random.random() < 0.8:  # 80% c√≥ t√™n ng∆∞·ªùi
                person_name = self.faker.name()
                record['CustomerName'] = person_name
                true_pii.append(('PERSON', person_name))
            
            if random.random() < 0.7:  # 70% c√≥ email
                email = self.faker.email()
                record['CustomerEmail'] = email  
                true_pii.append(('EMAIL_ADDRESS', email))
            
            if random.random() < 0.6:  # 60% c√≥ phone
                phone = self.faker.phone_number()
                record['CustomerPhone'] = phone
                true_pii.append(('PHONE_NUMBER', phone))
            
            if random.random() < 0.4:  # 40% c√≥ credit card
                cc = self.faker.credit_card_number()
                record['CreditCard'] = cc
                true_pii.append(('CREDIT_CARD', cc))
            
            if random.random() < 0.3:  # 30% c√≥ SSN
                ssn = self.faker.ssn()
                record['SSN'] = ssn
                true_pii.append(('US_SSN', ssn))
            
            # L∆∞u ground truth
            ground_truth[i] = true_pii
            records.append(record)
        
        return records, ground_truth
    
    def record_to_text_with_positions(self, record: Dict) -> Tuple[str, Dict]:
        """
        Chuy·ªÉn record th√†nh text v√† track v·ªã tr√≠ c·ªßa t·ª´ng field
        Returns: (text, {field_name: (start, end)})
        """
        text_parts = []
        field_positions = {}
        current_pos = 0
        
        for key, value in record.items():
            if value:
                field_text = f"{key}: {value}"
                start_pos = current_pos
                end_pos = current_pos + len(field_text)
                
                field_positions[key] = (start_pos, end_pos)
                text_parts.append(field_text)
                
                current_pos = end_pos + 3  # " | " separator
        
        full_text = " | ".join(text_parts)
        return full_text, field_positions
    
    def calculate_detection_accuracy(self, detected_entities: List, ground_truth_entities: List, text: str) -> Dict:
        """
        T√≠nh to√°n ƒë·ªô ch√≠nh x√°c detection d·ª±a tr√™n ground truth
        """
        # Convert ground truth to comparable format
        gt_set = set()
        for entity_type, value in ground_truth_entities:
            gt_set.add((entity_type, value))
        
        # Convert detected entities to comparable format  
        detected_set = set()
        for entity in detected_entities:
            entity_text = text[entity.start:entity.end]
            detected_set.add((entity.entity_type, entity_text))
        
        # Calculate metrics
        true_positives = len(gt_set.intersection(detected_set))
        false_positives = len(detected_set - gt_set)
        false_negatives = len(gt_set - detected_set)
        
        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            'true_positives': true_positives,
            'false_positives': false_positives, 
            'false_negatives': false_negatives,
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'ground_truth': list(gt_set),
            'detected': list(detected_set),
            'correct_detections': list(gt_set.intersection(detected_set)),
            'false_positives_list': list(detected_set - gt_set),
            'missed_entities': list(gt_set - detected_set)
        }
    
    async def evaluate_with_ground_truth(self, records: List[Dict], ground_truth: Dict, sample_size: int = 200) -> Dict:
        """ƒê√°nh gi√° v·ªõi ground truth"""
        print(f"\nüéØ Evaluating PII detection accuracy with ground truth...")
        
        total_tp = total_fp = total_fn = 0
        detailed_results = []
        
        for i in range(min(sample_size, len(records))):
            record = records[i]
            gt_entities = ground_truth.get(i, [])
            
            text, field_positions = self.record_to_text_with_positions(record)
            
            try:
                # Detect PII v·ªõi Presidio (raw)
                raw_results = self.masker.analyzer.analyze(
                    text=text,
                    language='en'
                )
                
                # Apply filtering ƒë·ªÉ c·∫£i thi·ªán accuracy
                filtered_results = self.filter_detections(raw_results, text)
                
                # Calculate accuracy for both raw and filtered
                raw_accuracy = self.calculate_detection_accuracy(raw_results, gt_entities, text)
                filtered_accuracy = self.calculate_detection_accuracy(filtered_results, gt_entities, text)
                
                # Use filtered results for overall metrics
                total_tp += filtered_accuracy['true_positives']
                total_fp += filtered_accuracy['false_positives']
                total_fn += filtered_accuracy['false_negatives']
                
                detailed_results.append({
                    'record_id': i,
                    'text': text[:150] + "..." if len(text) > 150 else text,
                    'ground_truth_count': len(gt_entities),
                    'raw_detected_count': len(raw_results),
                    'filtered_detected_count': len(filtered_results),
                    'raw_accuracy': raw_accuracy,
                    'filtered_accuracy': filtered_accuracy
                })
                
                if (i + 1) % 10 == 0:
                    print(f"  Processed {i + 1}/{sample_size} records")
                    
            except Exception as e:
                print(f"  ‚ùå Error processing record {i}: {e}")
                continue
        
        # Overall metrics
        overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0
        
        return {
            'overall_metrics': {
                'precision': overall_precision,
                'recall': overall_recall,
                'f1_score': overall_f1,
                'total_true_positives': total_tp,
                'total_false_positives': total_fp,
                'total_false_negatives': total_fn
            },
            'detailed_results': detailed_results
        }
    
    async def evaluate_masking_accuracy(self, records: List[Dict], sample_size: int = 50) -> Dict:
        """ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa masking/unmasking"""
        print(f"\nüîí Evaluating masking accuracy...")
        
        results = []
        perfect_roundtrips = 0
        successful_masks = 0
        
        for i in range(min(sample_size, len(records))):
            record = records[i]
            original_text, _ = self.record_to_text_with_positions(record)
            
            try:
                print(f"  Processing record {i + 1}/{sample_size}")
                
                # Step 1: Mask
                masked_text, mappings = await self.masker.mask_text(original_text)
                
                masking_successful = (masked_text != original_text) and bool(mappings)
                if masking_successful:
                    successful_masks += 1
                
                # Step 2: Unmask
                unmasked_text = original_text  # Default
                perfect_roundtrip = False
                
                if masking_successful and mappings:
                    unmasked_text = self.unmasker.unmask_text(masked_text, mappings)
                    perfect_roundtrip = (unmasked_text == original_text)
                    
                    if perfect_roundtrip:
                        perfect_roundtrips += 1
                
                # Calculate text similarity (for debugging)
                similarity = self.calculate_text_similarity(original_text, unmasked_text)
                
                results.append({
                    'record_id': i,
                    'original_length': len(original_text),
                    'masked_length': len(masked_text),
                    'entities_masked': len(mappings) if mappings else 0,
                    'masking_successful': masking_successful,
                    'perfect_roundtrip': perfect_roundtrip,
                    'text_similarity': similarity,
                    'sample_original': original_text[:100] + "...",
                    'sample_masked': masked_text[:100] + "...",
                    'sample_unmasked': unmasked_text[:100] + "..."
                })
                
            except Exception as e:
                print(f"  ‚ùå Error processing record {i}: {e}")
                results.append({
                    'record_id': i,
                    'error': str(e),
                    'masking_successful': False,
                    'perfect_roundtrip': False
                })
        
        total_processed = len([r for r in results if 'error' not in r])
        masking_success_rate = successful_masks / total_processed if total_processed > 0 else 0
        roundtrip_success_rate = perfect_roundtrips / total_processed if total_processed > 0 else 0
        
        return {
            'total_processed': total_processed,
            'successful_masks': successful_masks,
            'perfect_roundtrips': perfect_roundtrips,
            'masking_success_rate': masking_success_rate,
            'roundtrip_success_rate': roundtrip_success_rate,
            'detailed_results': results
        }
    
    def calculate_text_similarity(self, text1: str, text2: str) -> float:
        """T√≠nh ƒë·ªô t∆∞∆°ng t·ª± gi·ªØa 2 text (simple character-based)"""
        if not text1 or not text2:
            return 0.0
        
        # Simple character-level similarity
        common_chars = sum(1 for a, b in zip(text1, text2) if a == b)
        max_length = max(len(text1), len(text2))
        
        return common_chars / max_length if max_length > 0 else 0.0
    
    def save_detailed_report(self, detection_results: Dict, masking_results: Dict, filename: str = "accurate_pii_evaluation.json"):
        """L∆∞u b√°o c√°o chi ti·∫øt"""
        timestamp = datetime.now().isoformat()
        
        report = {
            'evaluation_type': 'Accurate PII Evaluation with Ground Truth',
            'timestamp': timestamp,
            'detection_accuracy': detection_results['overall_metrics'],
            'masking_accuracy': {
                'masking_success_rate': masking_results['masking_success_rate'],
                'roundtrip_success_rate': masking_results['roundtrip_success_rate'],
                'total_processed': masking_results['total_processed'],
                'successful_masks': masking_results['successful_masks'],
                'perfect_roundtrips': masking_results['perfect_roundtrips']
            },
            'detailed_detection_samples': detection_results['detailed_results'][:5],
            'detailed_masking_samples': masking_results['detailed_results'][:5]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"üìä Detailed report saved to {filename}")
        return report
    
    def print_accurate_summary(self, detection_results: Dict, masking_results: Dict):
        """In t√≥m t·∫Øt ch√≠nh x√°c"""
        print("\n" + "="*70)
        print("üéØ ACCURATE PII EVALUATION SUMMARY (With Ground Truth)")
        print("="*70)
        
        # Detection accuracy
        det_metrics = detection_results['overall_metrics']
        print(f"\nüîç PII DETECTION ACCURACY (Filtered):")
        print(f"   Precision:  {det_metrics['precision']:.3f} ({det_metrics['precision']*100:.1f}%)")
        print(f"   Recall:     {det_metrics['recall']:.3f} ({det_metrics['recall']*100:.1f}%)")
        print(f"   F1-Score:   {det_metrics['f1_score']:.3f} ({det_metrics['f1_score']*100:.1f}%)")
        print(f"   ‚úÖ True Positives:  {det_metrics['total_true_positives']}")
        print(f"   ‚ùå False Positives: {det_metrics['total_false_positives']}")
        print(f"   ‚ö†Ô∏è  False Negatives: {det_metrics['total_false_negatives']}")
        
        # Show improvement if we have comparison data
        if detection_results['detailed_results']:
            sample = detection_results['detailed_results'][0]
            if 'raw_accuracy' in sample and 'filtered_accuracy' in sample:
                raw_precision = sample['raw_accuracy']['precision']
                filtered_precision = sample['filtered_accuracy']['precision']
                improvement = ((filtered_precision - raw_precision) / raw_precision * 100) if raw_precision > 0 else 0
                print(f"\nüìà FILTERING IMPROVEMENT:")
                print(f"   Raw Precision:      {raw_precision:.3f} ({raw_precision*100:.1f}%)")
                print(f"   Filtered Precision: {filtered_precision:.3f} ({filtered_precision*100:.1f}%)")
                print(f"   Improvement:        {improvement:+.1f}%")
        
        # Masking accuracy
        print(f"\nüîí MASKING/UNMASKING ACCURACY:")
        print(f"   Masking Success Rate:    {masking_results['masking_success_rate']:.3f} ({masking_results['masking_success_rate']*100:.1f}%)")
        print(f"   Perfect Roundtrip Rate:  {masking_results['roundtrip_success_rate']:.3f} ({masking_results['roundtrip_success_rate']*100:.1f}%)")
        print(f"   Total Processed: {masking_results['total_processed']}")
        print(f"   ‚úÖ Successful Masks: {masking_results['successful_masks']}")
        print(f"   üéØ Perfect Roundtrips: {masking_results['perfect_roundtrips']}")
        
        # Overall assessment
        overall_score = (det_metrics['f1_score'] + masking_results['roundtrip_success_rate']) / 2
        
        # Calculate confidence intervals (simple approximation)
        total_samples = det_metrics['total_true_positives'] + det_metrics['total_false_positives'] + det_metrics['total_false_negatives']
        confidence_level = "High" if total_samples >= 200 else "Medium" if total_samples >= 100 else "Low"
        
        print(f"\nüèÜ OVERALL SYSTEM SCORE: {overall_score:.3f} ({overall_score*100:.1f}%)")
        print(f"\nüìä STATISTICAL RELIABILITY:")
        print(f"   Total Detection Samples: {total_samples}")
        print(f"   Total Masking Samples: {masking_results['total_processed']}")
        print(f"   Confidence Level: {confidence_level}")
        
        if total_samples < 100:
            print(f"   ‚ö†Ô∏è  Low sample size - consider running 'comprehensive' mode for higher reliability")
        elif total_samples < 200:
            print(f"   üìà Medium sample size - good for development testing")
        else:
            print(f"   ‚úÖ Large sample size - high statistical reliability")
        
        print("\n" + "="*70)


async def main():
    """Main evaluation function"""
    print("üéØ PII Performance Evaluation System (Final)")
    print("="*60)
    print("Features:")
    print("  ‚úÖ Ground Truth Dataset")
    print("  ‚úÖ Smart Filtering (False Positive Reduction)")  
    print("  ‚úÖ Precision/Recall/F1 Metrics")
    print("  ‚úÖ Masking/Unmasking Analysis")
    print("  ‚úÖ Detailed Error Debugging")
    print("  ‚úÖ Large Scale Testing (500 records, 200 detection samples, 50 masking samples)")
    print("="*60)
    
    evaluator = AccuratePIIEvaluator()
    
    # Step 1: Create labeled dataset
    print("\n1. Creating labeled dataset with ground truth...")
    records, ground_truth = evaluator.create_labeled_dataset(num_records=500)
    
    # Save dataset for inspection
    with open("labeled_dataset.json", 'w', encoding='utf-8') as f:
        json.dump({
            'records': records[:10],  # First 10 for inspection
            'ground_truth_sample': {str(k): v for k, v in list(ground_truth.items())[:10]}
        }, f, indent=2, ensure_ascii=False, default=str)
    
    # Step 2: Evaluate detection accuracy
    print("\n2. Evaluating PII detection accuracy...")
    detection_results = await evaluator.evaluate_with_ground_truth(records, ground_truth, sample_size=200)
    
    # Step 3: Evaluate masking accuracy  
    print("\n3. Evaluating masking/unmasking accuracy...")
    masking_results = await evaluator.evaluate_masking_accuracy(records, sample_size=50)
    
    # Step 4: Generate and save report
    print("\n4. Generating detailed report...")
    report = evaluator.save_detailed_report(detection_results, masking_results)
    
    # Step 5: Print summary
    evaluator.print_accurate_summary(detection_results, masking_results)
    
    print(f"\nüìä Results saved to: accurate_pii_evaluation.json")
    print(f"üìã Dataset saved to: labeled_dataset.json")
    print("\n‚úÖ PII Evaluation completed successfully!")
    
    return report


async def quick_test():
    """Quick test with smaller sample for fast feedback"""
    print("üöÄ Quick PII Evaluation (Small Sample)")
    print("="*50)
    
    evaluator = AccuratePIIEvaluator()
    
    # Small dataset for quick testing
    records, ground_truth = evaluator.create_labeled_dataset(num_records=50)
    detection_results = await evaluator.evaluate_with_ground_truth(records, ground_truth, sample_size=25)
    masking_results = await evaluator.evaluate_masking_accuracy(records, sample_size=10)
    
    evaluator.print_accurate_summary(detection_results, masking_results)
    return detection_results, masking_results


async def comprehensive_test():
    """Comprehensive test with large sample for maximum reliability"""
    print("üéØ Comprehensive PII Evaluation (Large Sample)")
    print("="*60)
    
    evaluator = AccuratePIIEvaluator()
    
    # Large dataset for comprehensive testing
    print("\nüìä This will take longer but provide more reliable results...")
    records, ground_truth = evaluator.create_labeled_dataset(num_records=1000)
    detection_results = await evaluator.evaluate_with_ground_truth(records, ground_truth, sample_size=500)
    masking_results = await evaluator.evaluate_masking_accuracy(records, sample_size=100)
    
    # Save comprehensive results
    report = evaluator.save_detailed_report(detection_results, masking_results, "comprehensive_pii_evaluation.json")
    evaluator.print_accurate_summary(detection_results, masking_results)
    
    print(f"\nüìä Comprehensive results saved to: comprehensive_pii_evaluation.json")
    return report


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "quick":
            asyncio.run(quick_test())
        elif sys.argv[1] == "comprehensive":
            asyncio.run(comprehensive_test())
        else:
            print("Usage: python accurate_pii_evaluation.py [quick|comprehensive]")
            print("  quick: Fast test with small sample (50 records, 25 detection, 10 masking)")
            print("  comprehensive: Thorough test with large sample (1000 records, 500 detection, 100 masking)")
            print("  default: Standard test (500 records, 200 detection, 50 masking)")
    else:
        asyncio.run(main())
